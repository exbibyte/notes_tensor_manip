\documentclass[8pt,letter]{article}
 
%% \usepackage[fleqn]{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage[ruled,vlined,linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subcaption}
%% \usepackage{datetime}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{parskip} %turns off paragraph indent
\pagestyle{fancy}

\usepackage{xcolor}
\usepackage{mdframed}

\usepackage[small]{titlesec}

\usetikzlibrary{arrows}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}

\newcommand{\mathleft}{\@fleqntrue\@mathmargin0pt}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}} 
\newcommand{\N}{\mathbb{N}}
\newcommand{\ppartial}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\p}{\partial}
\newcommand{\te}[1]{\text{#1 }}
\newcommand{\norm}[1]{\|#1\|}

\setcounter{MaxMatrixCols}{20}

% remove excess vertical space for align equations
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

% \newtheorem{mdtheorem}{Theorem}
% \newenvironment{theorem}
% {\begin{mdframed}[
%   backgroundcolor=green!10,
%   topline=false,
%   rightline=false,
%   bottomline=false,
%   leftline=false
%   ]\begin{mdtheorem}}
%     {\end{mdtheorem}\end{mdframed}}

\begin {document}

\lhead{Notes - Tensor/Array Manipulations, Bill (Yuan) Liu}

% \begin{align}
%   \nabla f_{k+1}^T p_k & \geq c_2 \nabla f_k^T p_k\\
%   \frac{\partial \phi(\alpha_k)}{\partial \alpha_k} & \geq \frac{\partial \phi(0)}{\partial \alpha_k}\\
%   &c_1,\alpha_k \in (0,1)\\
%   &0<c_1<c_2<1
% \end{align}

\begin{multicols*}{2}

  \section{Einops}

  Reference: https://github.com/arogozhnikov/einops
  
  \subsection{Features}
  
  \begin{itemize}
  \item self-documenting notation for layouts of input and output arrays
  \item low number of backend functions to implement
  \item focus on data rearrangements and simple transformations (axes reordering, decomposition, composition, reduction, repeats)
  \item focus on 1 tensor/array transformations
  \item notation uses strings
  \item supported notations: named axis, anonymous axis, unitary axis, ellipsis, (de)compose parenthesis
  \item supports a list of arrays as input with implied additional outer dimension corresponding to the list
  \item inferrable dimension sizes, given partial info as parameters
  \item hide backend framework inconsistency of notations for common array rearrangement operations
  \item use of proxy classes for specific backends
  \item caching of tensor type map to backend type for performance
  \item caching of patterns and axes
  \item caching of patterns, axes, and input shape: compute unknown axis sizes and shape verification on first time, otherwise reuse sequence of commands previously generated
  \item inverse transformations are easy to read off by switching patterns for input and output
  \end{itemize}

  \subsection{Approaches}
  
  \begin{itemize}
  \item evidence based for API design, via real world use cases
  \item explicit separation of a few functions over 1 function, for better error messages
  \item consideration of adoption friction and ease of use
  \end{itemize}

  \vfill\null
  \columnbreak
    
  \subsection{Known Issues}
  \begin{itemize}
  \item does not enforce axes alignment between operations
  \item no means of integrated analysis/tracking of shapes
  \end{itemize}
  
  \vfill\null
  \pagebreak

  \section{Tensor Derivatives}

  Index notation: $*(s_1,s_2,s_3)$, where\\
  $s_1$: input index set\\
  $s_2$: input index set\\
  $s_3$: output index set

  \subsection{Properties}

  \begin{itemize}
  \item associative
    
    let\\
    $ s_3 \subseteq s_1 \cup s_2$\\
    $s_4 \cap (s_1 \cup s_2) = \O$\\
    then,\\
    $*(s_3 s_4, s_4, s_3)(*(s_1, s_2 s_4, s_3 s_4)(A,B), C)\\
    =*(s_1, s_2, s_3)(A, *(s_2 s_4, s_4, s_2)(B,C))$

    order of evaluations:\\
    $(s_1 \rightarrow s_2 s_4) \rightarrow s_4 \rightarrow s_3$\\
    vs\\
    $s_1 \rightarrow (s_2 s_4 \rightarrow s_4) \rightarrow s_3$
     
    \item commutative

      $(s_1,s_2,s_3)(A,B) = *(s_2,s_1,s_3)(B,A)$
    
    \item distributive

      $*(s_1,s_2,s_3)(A,B) + *(s_1,s_2,s_3)(A,C)\\= *(s_1,s_2,s_3)(A, B+C)$\\
      where $s_3 \subseteq s_1 \cup s_2$

  \end{itemize}

  \subsection{Derivative Definition}

  $f: \R^{n_1 \times .. \times .. n_k} \rightarrow \R^{m_1 \times .. \times m_l}$\\
  $D \in \R^{m_1 \times .. \times m_l \times n_1 \times .. \times n_k}$

  $\lim_{h \rightarrow 0} \frac{\| f(x+h) - f(x) - D \circ h\|}{\| h \|} = 0$\\
  $\iff D$ is a derivative of $f$ at $x$\\
  
  where inner tensor product is: \\
  $D \circ h = *(s_1 s_2, s_2, s_1)(D,h)$

  \vfill\null
  \columnbreak
    
  \section{Forward Mode}
  $\sum_{i} \frac{ \partial v_i}{\partial x_j} \frac{\partial f}{\partial x_j} = \frac{\partial f}{\partial x_j}$\\
  where $x_j$ are leaf input variables and\\
  where pushforwards of predecessor nodes $v_i$ are computed and cached by the time $\frac{\partial f}{\partial x_j}$ is computed\\
  
  notation: let $\bar{v} = \frac{\partial v}{\partial x_j}$ be the pushforward of $v$

  Generalized cases of local node connections:
  \begin{itemize}
  \item unary function
  \item element-wise unary function
  \item binary addition
  \item binary multiplication
  \end{itemize}

  We seek to compute pushforwards for the above types of ops.

  \subsection{Pushforward of Unary Function}
  todo

  \subsection{Pushforward of Elementwise Unary Function}
  todo

  \subsection{Binary Addition}
  todo

  \subsection{Binary Multiplication}
  todo

  \vfill\null
  \columnbreak
    
  \section{Reverse Mode}
  todo
  
\end{multicols*}
\end {document}


